# Observations concerning performance of flannel as a CNI in microshift

## AIM

Measure the CPU and memory utilisation of flannel running as a CNI in microshift as an application deployed over the latter scales up and down, subsequently increasing the workload on flannel when it comes to networking.

## Testing setup

https://github.com/openshift/microshift/blob/main/docs/devenv_rhel8.md serves as a guide for building and running microshift.
A 2 core, 4G RAM VM created with RHEL8 ISO and the recommended configurations done. Intention is to keep the resource footprint of measuring application small to ensure that it doesn't overwhelm the VM. Post some initial testing with utilities specifically constructed for performance measurement like `podman stats`, `ps` etc.; `nmon`, `top`, `kubectl top` have been chosen for their clear, precise presentation of memory and CPU statistics, which can be represented easily in graphical format to observe the trend; and for their low resource consumption ability.


## Testing strategy 

Scripts created to allow for consistency when it comes to different tests and to allow for mapping of event in utilisation graphs. These scripts consist of triggering start of microshift, creating a nginx deployment with container resource limits of "100Mi" for memory and "50m" for cpu. Following this, the deployment has been scaled up and down drastically (upto max number of replicas it can reach) to observe flannel when it is actually at play. During this scaling process, a service is attached to the deployment of type 'NodePort' so that load can be put onto the replicas, post the scaling activity. Load has been generated by simply running a script, essentially a loop with curl commands hitting the nginx service, with 12 threads triggered from outside the VM.

Same testing was repeated with half the aforementioned resource limit of containers to allow for larger scaling.

## Observations/ Results

#### Test-1:
1. Trigger recording of statistics with nmon and top (2s gap)
2. Start microshift
3. Scale upto 28 replicas, up-down repeatedly (around 12 times). Post this scaling phase- nginx pods are at their max count in terms of resources of the server.
4. Bombard with external load (15:30:07 to 15:32:05) - Steps 3,4 are exclusive in nature i.e. don't happen simultaneously for the purpose of this testing.
*Recorded in file toptest1-2s.html and topdata1-2s.txt*

![CPU Utilisation Percentages-1](images/CPU-Test1.png?raw=true "CPU Utilisation Percentages-1")

#### Test-2:
1. Start microshift
2. Trigger recording of statistics with nmon and top (1s gap)
3. Scale upto 28 replicas, up-down repeatedly (around 12 times)
4. Bombard with external load
*Recorded in file toptest2-1s.html and topdata2-1s.txt*

![CPU Utilisation Percentages-2](images/CPU-Test2.png?raw=true "CPU Utilisation Percentages-2")

#### Test-3:
1. Trigger recording of statistics with nmon and top (1s gap)
2. Start microshift
3. Scale upto 28 replicas, up-down repeatedly (around 12 times)
4. Bombard with external load (16:50:41 to 16:56:51)
*Recorded in file toptest3-1s.html and topdata3-1s.txt*

![CPU Utilisation Percentages-3](images/CPU-Test3.png?raw=true "CPU Utilisation Percentages-3")

**Observations:**

At times CPU utilisation is pretty close to 100% especially during the scaling phase. The peaks in nmon file are attributed to the microshift process when related with data from the top command. kswapd0 (process concerning virtual memory, swap space) also seems to be consuming a lot of CPU resources- A later test with swap off at start of test also didn't eliminate its usage. Overall the CPU utilisation pattern is pretty jagged, and flanneld process in particular lies in ranges `0.1-0.5` i.e. `1% to 5%` CPU consumption. Memory consumption for flanneld results in quite a flat curve with value ranging in `25MB to 31MB`. Overall memory consumption of the system lies around 2250MB +/-100MB; irrespective of phase of the testing process even though at its peak, ~7150 requests were being made to the service per minute, per thread.

Disk graphs have been collected as well to have the pattern and peak values of how busy different disks are to have as a reference value for comparison with that of patu CNI; and most peaks range mainly around 75-77% across all the tests, and occasionally touch 100%.

##### Flannel CPU(%)/Memory(MB) consumption trend
###### (This pattern was observed using various tests- please refer files in google drive[links below] for further details.)
![Flannel CPU/Memory consumption](images/FlannelData.png?raw=true "Flannel CPU/Memory consumption")

##### Disk Data across tests
![Disk Data across tests](images/DiskData.png?raw=true "Disk Data across tests")

### Additional testing
Attached excel sheets note the graphical trend of CPU consumption, peaks of memory consumption of each of the pods that microshift uses measured using metric-server of kubernetes which will serve as a base for comparison when microshift will be deployed with patu as its CNI. These measurements has been done on container basis by getting their container Ids vis `crictl`, and experimenting with how the graphs differ when load is bombarded from external/ internal sources. 

*Files in Google Drive:*
https://drive.google.com/drive/folders/1QeSnN-l-5hPep_eZpZFvU5X7G2ZhK8vZ?usp=sharing
https://docs.google.com/spreadsheets/d/10GZAe4TfW93xL6aEpcxnX-73I7YGfww5U05Go9HcEds/edit?usp=sharing
https://docs.google.com/spreadsheets/d/1Pj6M__fh_cn7hII6WEDY-iW4l5suJTs-w42czmrm5YU/edit?usp=sharing 
